# -*- coding: utf-8 -*-
"""Autism_Predication_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p7-5_WMl99sL0wNABfXbtPKtspuBmKuW

**1.Importing the dependencies**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score,RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pickle

"""**2. Data Loading & Understading**

"""

df=pd.read_csv("/content/train.csv")

"""**Initial Inspection**"""

df.shape

df.head()

df.tail(2)

# to display all the columns of a dataframe
pd.set_option('display.max_columns',None)

df.info()

#rename the column name from contry_of_res to country_of_res and austim to autism
df.rename(columns={"contry_of_res":"country_of_res"},inplace=True)
df.rename(columns={"austim":"autism"},inplace=True)

#convert age column datatype to integer
df["age"]=df["age"].astype(int)

df.info()

df.head(2)

for col in df.columns :
  numerical_features =["ID","age","result"]
  if col not in numerical_features:
    print(col,df[col].unique())
    print("-"*60)

#dropping the ID and age_desc column
df = df.drop(columns=["ID","age_desc"])

df.shape

df.head(2)

df.columns

df["country_of_res"].unique()

from collections.abc import Mapping
#define the mapping dictionary for country names
mapping ={
    "Viet Nam":"vietnam",
    "AmericanSamoa":"United States",
    "Hong Kong":"China"
}

#replace value in the country column

df["country_of_res"]=df["country_of_res"].replace(mapping)

df["country_of_res"].unique()

#target class distribution

df["Class/ASD"].value_counts()

"""**Insight get from step 2**

1.missing values in ethnicity & relation

2.age_desc column has only 1 unique value.so it removed as it is not important for  prediction

3.fixed country name

4.identified class imbalance in the target column

**3.Exploratory Data Analysis(EDA)**
"""

df.describe()

"""**Univariate Analysis**

Numerical Columns:
- age
- result


"""

#set the desired theme using seaborn
sns.set_theme(style="darkgrid")

#Histogram for age
sns.histplot(df["age"],kde=True)
plt.title("Distribution of Age")

#calculate mean and median
age_mean=df["age"].mean()
age_median=df["age"].median()

print("Mean :",age_mean)
print("Median :",age_median)

#add vertical lines for mean and median
plt.axvline(age_mean,color="red",linestyle="--",label="Mean")
plt.axvline(age_median,color="yellow",linestyle="-",label="Median")
plt.legend()
plt.show()

#Histogram for result
sns.histplot(df["result"],kde=True)
plt.title("Distribution of Result")

#calculate mean and median
result_mean=df["result"].mean()
result_median=df["result"].median()

print("Mean :",result_mean)
print("Median :",result_median)

#add vertical lines for mean and median
plt.axvline(result_mean,color="red",linestyle="--",label="Mean")
plt.axvline(result_median,color="yellow",linestyle="-",label="Median")
plt.legend()
plt.show()

"""**Box plots for identifying outliers in the numerical columns**"""

#Box plot
sns.boxplot(x=df["age"])
plt.title("Box plot for Age")
plt.xlabel("Age")

plt.show()

#Box plot
sns.boxplot(x=df["result"])
plt.title("Box plot for Result")
plt.xlabel("Result")
plt.show()

#count the outliers using the IQR method
Q1=df["age"].quantile(0.25)
Q3=df["age"].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-(1.5*IQR)
upper_bound=Q3+(1.5*IQR)

age_outlier=df[(df["age"]<lower_bound) | (df["age"]>upper_bound)]
len(age_outlier)

#count the outliers using the IQR method
Q1=df["result"].quantile(0.25)
Q3=df["result"].quantile(0.75)
IQR=Q3-Q1
lower_bound=Q1-(1.5*IQR)
upper_bound=Q3+(1.5*IQR)

result_outlier=df[(df["result"]<lower_bound) | (df["result"]>upper_bound)]
len(result_outlier)

"""**Univariate Analysis of Categorical columns**"""

df.columns

categorical_columns=['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score',
       'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'gender',
       'ethnicity', 'jaundice', 'autism', 'country_of_res', 'used_app_before',
        'relation']

for col in categorical_columns:
  # plt.figure(figsize=(20,5))
  sns.countplot(x=df[col])
  plt.title(f"Count plot fot {col}")
  plt.xlabel(col)
  plt.ylabel("Count")
  plt.show()

#count plot for target column(Class/ASD)
sns.countplot(x=df["Class/ASD"])
plt.title("Count plot for Class/ASD")
plt.xlabel("Class/ASD")
plt.ylabel("Count")
plt.show()

df["Class/ASD"].value_counts()

"""**handle missing values in ethnicity and relation column**"""

df["ethnicity"]=df["ethnicity"].replace({"?":"Others","others":"Others"})

df["ethnicity"].unique()

df["relation"].unique()

df["relation"]=df["relation"].replace({"?":"Others",
                                       "Relative":"Others",
                                       "Parent":"Others",
                                       "Health care professional":"Others"})

df["relation"].unique()

df.head()

"""**Label Encoding**"""

#identify columns with object data type
object_columns=df.select_dtypes(include="object").columns

print(object_columns)

#initialize a dictionary to store the encoders
encoders={}
#apply label encoding and store the encoders
for column in object_columns:
  label_encoder=LabelEncoder()
  df[column]=label_encoder.fit_transform(df[column])
  encoders[column]=label_encoder #saving the encoder for this column

#save the encoders as a pickle file
with open("encoders.pkl","wb") as f:
  pickle.dump(encoders,f)

encoders

df.head()

"""**Bivariate Analysis**"""

#correlaltion matrix
 plt.figure(figsize=(15,6))
 sns.heatmap(df.corr(),annot=True,cmap="coolwarm",fmt=".2f")
 plt.title("Correlation heatmap")
 plt.show()

"""**Insights we got from the EDA**
- there are few outliers in the numerical column(age,result)
- there is class imbalance in target column
- there is class imbalance in categorical feature
- we don't have any highly correlated column
- perform label encoding and saved the encodes

**4. Data Preprocessing**

Handling the outliers
"""

#function to replace the outliers with median
def replace_outliers_with_median(df,column):
  Q1=df[column].quantile(0.25)
  Q3=df[column].quantile(0.75)
  IQR=Q3-Q1
  lower_bound=Q1-(1.5*Q1)
  upper_bound=Q3+(1.5*Q3)

  median=df[column].median()

  #replace outliers with median value
  df[column]=df[column].apply(lambda x:median if x<lower_bound or x>upper_bound else x)

  return df

#replace outliers in the "age " column
df=replace_outliers_with_median(df,"age")

#replace outliers in the "result " column
df=replace_outliers_with_median(df,"result")

df.head()

df.shape

"""**Train Test Split**"""

df.columns

X=df.drop(columns=["Class/ASD"])
y=df["Class/ASD"]

print(X)

print(y)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

print(y_train.shape)
print(y_test.shape)

y_train.value_counts()

y_test.value_counts()

"""SMOTE(Synthetic minority oversampling technique)"""

smote=SMOTE(random_state=42)
X_train_smote,y_train_smote=smote.fit_resample(X_train,y_train)

print(y_train_smote.shape)

print(y_train_smote.value_counts())

"""**5.Model Training**"""

#dictionary of classifiers
models={
    "Decision Tree":DecisionTreeClassifier(random_state=42),
    "Random Forest":RandomForestClassifier(random_state=42),
    "XGBoost":XGBClassifier(random_state=42)
}

print(models)

#dictionary to store the cross-validation for each model
cv_scores={}

#perform 5-fold cross-validation for each model
for model_name,model in models.items():
  print(f"Training {model_name} with default parameters...")
  scores=cross_val_score(model,X_train_smote,y_train_smote,cv=5,scoring="accuracy")
  cv_scores[model_name]=scores
  print(f"{model_name} Cross-Validation Accuracy:{np.mean(scores):.2f}")
  print("-"*60)

cv_scores

"""**6.Model Selection and Hyperparameter Tuning**"""

#Initializing models
decision_tree=DecisionTreeClassifier(random_state=42)
random_forest=RandomForestClassifier(random_state=42)
xgboost_classifier=XGBClassifier(random_state=42)

# Hyperparameters grid for RandomizedSearchCV

param_grid_dt={
  "criterion":["gini","entropy"],
  "max_depth":[None,5,10,15,20,25,50],
  "min_samples_split":[2,5,10],
  "min_samples_leaf":[1,2,4]

}

param_grid_rf={
    "n_estimators":[50,100,150,200],
    "max_depth":[None,5,10,20],
    "min_samples_split":[2,5,10],
    "min_samples_leaf":[1,2,4],
    "bootstrap":[True,False]
}

param_grid_xgb={
    "n_estimators":[50,100,200,700],
    "max_depth":[None,5,10,30,50],
    "learning_rate":[.01,0.1,0.2,0.3],
    "subsample":[0.5,0.7,1.0],
    "colsample_bytree":[0.5,0.7,1.0]


}

#hyperparameter tuning for 3 tree based models
#the below steps can be automated by using a for loop or by using a pipeline
#perform RandomizedSearchCV for each model

random_search_dt=RandomizedSearchCV(estimator=decision_tree,param_distributions=param_grid_dt,n_iter=20,cv=5,scoring="accuracy",random_state=42)
random_search_rf=RandomizedSearchCV(estimator=random_forest,param_distributions=param_grid_rf,n_iter=20,cv=5,scoring="accuracy",random_state=42)
random_search_xgb=RandomizedSearchCV(estimator=xgboost_classifier,param_distributions=param_grid_xgb,n_iter=20,cv=5,scoring="accuracy",random_state=42)

#fit the models
random_search_dt.fit(X_train_smote,y_train_smote)
random_search_rf.fit(X_train_smote,y_train_smote)
random_search_xgb.fit(X_train_smote,y_train_smote)

print(random_search_dt.best_estimator_)
print(random_search_dt.best_score_)

print(random_search_rf.best_estimator_)
print(random_search_rf.best_score_)

#get the model with the best score

best_model=None
best_score=0

if random_search_dt.best_score_>best_score:
  best_model=random_search_dt.best_estimator_
  best_score=random_search_dt.best_score_

if random_search_rf.best_score_>best_score:
  best_model=random_search_rf.best_estimator_
  best_score=random_search_rf.best_score_

if random_search_xgb.best_score_>best_score:
  best_model=random_search_xgb.best_estimator_
  best_score=random_search_xgb.best_score_

print(f"Best Model: {best_model}")
print(f"Best Cross-validation Accuracy:{best_score:.2f}")

with open("best_model.pkl","wb") as f:
  pickle.dump(best_model,f)

#evaluate on the global test data before we perform smote
y_test_pred=best_model.predict(X_test)
print("Accuracy Score:", accuracy_score(y_test,y_test_pred))
print("Confusion Matrix:",confusion_matrix(y_test,y_test_pred))
print("Classificationn Report:",classification_report(y_test,y_test_pred))

